<!DOCTYPE html>
<html>
<head>
  <script src="face-api.js"></script>
  <script src="FileSaver.js"></script>
  <script src="commons.js"></script>
  <script src="js/faceExpressionsCommons.js"></script>
</head>
<body>
  <div id="container"></div>
  <div id="template" style="display: inline-flex; flex-direction: column;">
    <span class="faceExpression-text"></span>
    <span class="predicted-text"></span>
  </div>

  <script>
    tf = faceapi.tf

    // load the FaceLandmark68Net and use it's feature extractor since we only
    // train the output layer of the FaceExpressionNet
    const dummyLandmarkNet = new faceapi.FaceLandmark68Net()
    window.net = new faceapi.FaceExpressionNet(dummyLandmarkNet.faceFeatureExtractor)

    // uri to weights file of last checkpoint
    const modelCheckpoint = 'tmp/full/initial_glorot.weights'
    const startEpoch = 0

    const learningRate = 0.001
    window.optimizer = tf.train.adam(learningRate, 0.9, 0.999, 1e-8)

    window.batchSize = 8

    window.iterDelay = 0
    window.withLogging = true

    async function load() {
      window.trainData = await faceapi.fetchJson('trainData.json')

      // fetch the actual output layer weights
      const weights = await faceapi.fetchNetWeights(modelCheckpoint)
      await window.net.load(weights)
      window.net.faceFeatureExtractor.variable()
      window.net.variable()
    }

    async function train() {
      await load()

      const shuffledInputs = prepareDataForEpoch(window.trainData).slice(0, window.batchSize)
      const batchData = shuffledInputs
      const bImages = await Promise.all(
        batchData
          .map(data => getImageUrl(data))
          .map(imgUrl => faceapi.fetchImage(imgUrl))
      )
      const bOneHotVectors = batchData
        .map(data => getLabelOneHotVector(data.label))

      const container = document.getElementById('container')
      const template = document.getElementById('template')

      bImages.forEach((img, i) => {
        console.log(i, batchData[i].label, batchData[i].img)

        const squaredImg = faceapi.imageToSquare(img, 112, true)
        const faceExpressions = faceapi.FaceExpressionNet
          .decodeProbabilites(bOneHotVectors[i])
          .filter(e => e.probability > 0)

        const clone = template.cloneNode(true)
        clone.id = i
        const span = clone.firstElementChild
        span.innerHTML = i + ':' + faceExpressions[0].expression
        clone.insertBefore(squaredImg, span)
        container.appendChild(clone)
      })

      for (let epoch = startEpoch; epoch < Infinity; epoch++) {

        const netInput = await faceapi.toNetInput(bImages)

        const loss = optimizer.minimize(() => {
          const labels = tf.tensor2d(bOneHotVectors)
          const out = window.net.runNet(netInput)

          const loss = tf.losses.softmaxCrossEntropy(
            labels,
            out,
            tf.Reduction.MEAN
          )

          const predictedByBatch = tf.unstack(tf.softmax(out))
          predictedByBatch.forEach((p, i) => {
            const probabilities = Array.from(p.dataSync())
            const faceExpressions = faceapi.FaceExpressionNet.decodeProbabilites(probabilities)
            const container = document.getElementById(i)

            const pred = faceExpressions.reduce((best, curr) => curr.probability > best.probability ? curr : best)

            const predNode = container.children[container.children.length - 1]

            predNode.innerHTML =
              pred.expression + ' (' +  faceapi.round(pred.probability) + ')'
          })

          return loss
        }, true)

        // start next iteration without waiting for loss data

        loss.data().then(data => {
          const lossValue = data[0]
          log(`epoch ${epoch}, loss: ${lossValue}`)
          loss.dispose()
        })

        if (window.iterDelay) {
          await delay(window.iterDelay)
        } else {
          await tf.nextFrame()
        }
      }
    }

  </script>
</body>
</html>